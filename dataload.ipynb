{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import  DataLoader\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetAudio(data.Dataset):\n",
    "    def __init__(self, dset_path, max_lengh=10000, seq_lengh=15, is_train=True):\n",
    "        super(DataSetAudio).__init__()\n",
    "        self.dset_path, self.labels = self.extract(dset_path)\n",
    "        self.max_lengh = max_lengh\n",
    "        self.seq_lengh = seq_lengh\n",
    "        self.is_train = is_train\n",
    "        self.steps = int(SAMPLE_RATE*30/seq_lengh)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.max_lengh\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Same validation\n",
    "        if not self.is_train:\n",
    "            random.seed(72)\n",
    "        return self\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.dset_path[idx%999]\n",
    "        label = audio_path.split('/')[2]\n",
    "        signal, sr = librosa.load(audio_path, sr = SAMPLE_RATE)\n",
    "        #choosing random part of the songs\n",
    "        gen = random.randrange(50,len(signal) - self.steps)\n",
    "        mfcc = librosa.feature.mfcc(y = signal[gen: gen + self.steps],\n",
    "                                                    sr = sr,\n",
    "                                                    n_fft = 2048,\n",
    "                                                    n_mfcc = 13,\n",
    "                                                    hop_length = 512)\n",
    "        return mfcc.T, self.labels[label]\n",
    "\n",
    "    def extract(self,dir):\n",
    "        file_list = []\n",
    "        labels = defaultdict()\n",
    "        for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dir)):\n",
    "                if dirpath is not dir:\n",
    "                        labels[dirpath.split('/')[-1]] = i-1\n",
    "                        for file in filenames:\n",
    "                                file_list.append(os.path.join(dirpath, file))\n",
    "        #todo add len of dir\n",
    "        return file_list, labels\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/genres_original'\n",
    "dataset = DataSetAudio(dset_path=data_path)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_samples(dloader):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for batch in tqdm((dloader)):\n",
    "        inputs, labels = batch\n",
    "        x_train.append(inputs)\n",
    "        y_train.append(labels)\n",
    "\n",
    "    x_train = np.concatenate(x_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:57<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = gen_training_samples(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 1000,\n",
       "         5: 1000,\n",
       "         8: 1000,\n",
       "         7: 1000,\n",
       "         0: 1010,\n",
       "         1: 1000,\n",
       "         9: 1000,\n",
       "         4: 990,\n",
       "         2: 1000,\n",
       "         6: 1000})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Validate equal parts\n",
    "'''\n",
    "from collections import Counter\n",
    "\n",
    "Counter(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train.npy',X_train)\n",
    "np.save('Y_train.npy',Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:34<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "dsetest = DataSetAudio(dset_path=data_path,max_lengh=2000)\n",
    "dloadertest = DataLoader(dsetest, batch_size=32, shuffle=True)\n",
    "X_test, Y_test = gen_training_samples(dloadertest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_test.npy',X_test)\n",
    "np.save('Y_test.npy',Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
